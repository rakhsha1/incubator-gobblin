job.name=AliLocal
job.group=AliLocal
job.description=Backup events from Kafka to local file.
job.lock.enabled=true

kafka.brokers=localhost:9092

source.class=org.apache.gobblin.source.extractor.extract.kafka.KafkaSimpleSource
extract.namespace=gobblin.extract.kafka

writer.file.path.type=tablename
writer.destination.type=HDFS
writer.output.format=PARQUET
writer.builder.class=org.apache.gobblin.writer.ParquetDataWriterBuilder

writer.staging.dir=${env:GOBBLIN_WORK_DIR}/task-staging
writer.output.dir=${env:GOBBLIN_WORK_DIR}/task-output

writer.codec.type=gzip
#writer.file.block.size

launcher.type=LOCAL
type=java
job.metrics.host=localhostclass=org.apache.gobblin.azkaban.AzkabanJobLauncher

data.publisher.type=org.apache.gobblin.publisher.BaseDataPublisher
data.publisher.final.dir=${env:GOBBLIN_WORK_DIR}/job-output

mr.job.max.mappers=8

metrics.reporting.file.enabled=true
metrics.log.dir=${env:GOBBLIN_WORK_DIR}/metrics
metrics.reporting.file.suffix=txt

bootstrap.with.offset=latest

reset.on.offset.out.of.range=nearest

topic.whitelist=events

converter.classes=org.apache.gobblin.converter.parquet.PGJsonEventConverter,org.apache.gobblin.converter.parquet.JsonIntermediateToParquetGroupConverter

source.schema="[{"columnName":"timestamp","dataType":{"type":"long"}},{"columnName":"value","dataType":{"dataType":"string"}}]"

metrics.enabled=false
metrics.host=localhost
metrics.port=9199
