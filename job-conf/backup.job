job.name=AliLocal
job.group=AliLocal
job.description=Backup events from Kafka to local file.
job.lock.enabled=true

kafka.brokers=localhost:9092

source.class=org.apache.gobblin.source.extractor.extract.kafka.KafkaSimpleSource
extract.namespace=gobblin.extract.kafka

launcher.type=LOCAL
type=java
job.metrics.host=localhostclass=org.apache.gobblin.azkaban.AzkabanJobLauncher

mr.job.max.mappers=8

metrics.reporting.file.enabled=true
metrics.log.dir=${env:GOBBLIN_WORK_DIR}/metrics
metrics.reporting.file.suffix=txt

bootstrap.with.offset=latest

reset.on.offset.out.of.range=nearest

topic.whitelist=events,location

fork.operator.class=org.apache.gobblin.fork.PGLocationBasedForkOperator

#for normal fork
converter.classes.0=org.apache.gobblin.converter.parquet.PGJsonEventConverter,org.apache.gobblin.converter.parquet.JsonIntermediateToParquetGroupConverter

writer.file.path.type.0=tablename
writer.destination.type.0=HDFS
writer.output.format.0=PARQUET
writer.builder.class.0=org.apache.gobblin.writer.ParquetDataWriterBuilder
writer.staging.dir.0=${env:GOBBLIN_WORK_DIR}/task-staging
writer.output.dir.0=${env:GOBBLIN_WORK_DIR}/task-output
writer.codec.type.0=gzip

data.publisher.type.0=org.apache.gobblin.publisher.BaseDataPublisher
data.publisher.final.dir.0=${env:GOBBLIN_WORK_DIR}/job-output

#for filtered fork
converter.classes.1=org.apache.gobblin.converter.parquet.PGJsonEventConverter,org.apache.gobblin.converter.parquet.JsonIntermediateToParquetGroupConverter

writer.file.path.type.1=tablename
writer.destination.type.1=HDFS
writer.output.format.1=PARQUET
writer.builder.class.1=org.apache.gobblin.writer.ParquetDataWriterBuilder
writer.staging.dir.1=${env:GOBBLIN_WORK_DIR}/task-staging/geo-filtered
writer.output.dir.1=${env:GOBBLIN_WORK_DIR}/task-output/geo-filtered
writer.codec.type.1=gzip

data.publisher.type.1=org.apache.gobblin.publisher.BaseDataPublisher
data.publisher.final.dir.1=${env:GOBBLIN_WORK_DIR}/job-output/geo-filtered

topics.location=location
location.precision=3
location.filter.enabled=true

source.schema="[{"columnName":"timestamp","dataType":{"type":"long"}},{"columnName":"value","dataType":{"dataType":"string"}}]"

metrics.enabled=false
metrics.host=localhost
metrics.port=9199
